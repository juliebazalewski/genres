{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Genre Sentiment Analysis\n",
    "\n",
    "Julie Bazalewski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "#https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.TextBlob.sentiment\n",
    "from textblob import TextBlob\n",
    "#https://pypi.org/project/demoji/\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set up Twitter Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/twitter_credentials.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_list,num_needed):\n",
    "    \n",
    "    \"\"\"\n",
    "   Obtains specified number of tweets from Twitter for given search terms\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_list: list\n",
    "    List of search terms\n",
    "    \n",
    "    num_needed: int\n",
    "    Minimum number of desired tweets for each search term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    full_tweet_list: list\n",
    "    A nested list containing tweet data for each search term\n",
    "    \"\"\"\n",
    "\n",
    "    full_tweet_list = []\n",
    "\n",
    "    for i in range(len(search_list)):  #loop over number of search terms\n",
    "    \n",
    "        tweet_list = []  #reset list\n",
    "        last_id = -1   # id of last tweet seen\n",
    "        print('running: {}'.format(search_list[i])) #print search iteration\n",
    "    \n",
    "        while len(tweet_list) < num_needed:   #while number of tweets is below desired amount:\n",
    "            try:                              #try to obtain 100 more tweets\n",
    "                new_tweets = api.search(q=search_list[i], lang='en', tweet_mode='extended', \\\n",
    "                                        count = 100, max_id = str(last_id - 1))\n",
    "\n",
    "            except tweepy.TweepError as e:  #print error if reach limit\n",
    "                print(\"Error\", e)\n",
    "                break\n",
    "        \n",
    "            else:\n",
    "                if not new_tweets: #print error if not enough tweets can be obtained\n",
    "                    print(\"Could not find any more tweets!\")\n",
    "                    break\n",
    "                \n",
    "                tweet_list.extend(new_tweets) #add new tweets from this iteration to list\n",
    "                last_id = new_tweets[-1].id\n",
    "    \n",
    "        full_tweet_list.append(tweet_list)  #append all tweets for current search term to new position in full list\n",
    "        \n",
    "    return full_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_dict_pop(full_tweet_list,search_terms):\n",
    "    \n",
    "    \"\"\"\n",
    "    Populates dictionary with relevant tweet data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tweet_list: list\n",
    "    Nested list of tweet data for each search term\n",
    "    \n",
    "    search_terms: list\n",
    "    List of search term names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tweet_dict: dictionary\n",
    "    A nested dictionary containing text, username, and \n",
    "    location data for each tweet for each search term\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet_dict = {}\n",
    "    \n",
    "    for i in range(len(search_terms)): #for each search term:\n",
    "        \n",
    "        search_term = search_terms[i]\n",
    "        \n",
    "        if search_term not in tweet_dict:   #if key does not exist:\n",
    "            tweet_dict[search_term] = {}    # initialize key\n",
    "        for tweet in full_tweet_list[i]:  #for each tweet:\n",
    "        \n",
    "            if 'text' not in tweet_dict[search_term]:   #if key does not exist:\n",
    "                tweet_dict[search_term]['text'] = []    # initialize key\n",
    "            tweet_dict[search_term]['text'].append(tweet.full_text)  #append tweet text to key 'text'\n",
    "        \n",
    "            if 'user' not in tweet_dict[search_term]:  #if key does not exist:\n",
    "                tweet_dict[search_term]['user'] = []     # initialize key\n",
    "            tweet_dict[search_term]['user'].append(tweet.user.screen_name) #append username to key 'user'       \n",
    "        \n",
    "            if 'location' not in tweet_dict[search_term]:  #if key does not exist:\n",
    "                tweet_dict[search_term]['location'] = []   # initialize key\n",
    "            tweet_dict[search_term]['location'].append(tweet.user.location) #append location to key 'location'   \n",
    "            \n",
    "        tweet_dict[search_term]['genre'] = search_term #add genre key to each nested dict\n",
    "        \n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(tweet_dict,search_terms):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates Pandas data frame from nested dictionary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tweet_dict: dictionary\n",
    "    Nested dictionary of tweet data\n",
    "    \n",
    "    search_terms: list\n",
    "    List of search term names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tweet_df: data frame\n",
    "    Data Frame converted from input dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet_df = pd.DataFrame()\n",
    "    \n",
    "    for search_term in search_terms:\n",
    "        tweet_df = tweet_df.append(pd.DataFrame(tweet_dict[search_term]))\n",
    "\n",
    "    tweet_df = pd.DataFrame.reset_index(tweet_df)\n",
    "    tweet_df = tweet_df.drop('index', axis=1)\n",
    "    \n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punct(s):\n",
    "    \n",
    "    \"\"\"\n",
    "    Removes punctuation from a string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s: string\n",
    "    String to be cleaned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s: string\n",
    "    Cleaned string\n",
    "    \"\"\"\n",
    "        \n",
    "    new_s = []\n",
    "    #check if each character is punctuation. If not, append to new list\n",
    "    [new_s.append(s[i]) for i in range(len(s)) if s[i] not in list(string.punctuation + \"‘’”“…←♬\")]\n",
    "    \n",
    "    #flatten new, cleaned list and save to original string\n",
    "    s=''.join(new_s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(s):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans a given string. Removes URLS, emojis, EOL characters, punctuation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s: string\n",
    "    String to be cleaned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s: string\n",
    "    Cleaned string\n",
    "    \"\"\"\n",
    "        \n",
    "    s = re.sub(r'http\\S+', ' ', s)  #remove urls\n",
    "    s = re.sub(r'@[A-Za-z0-9_]+', ' ', s) #remove mentions, usernames can contain letters, numbers, and underscore\n",
    "    s = demoji.replace(s)  #remove emojis \n",
    "    s = s.replace('\\n', ' ').replace('\\t',' ').replace('  ',' ')  #remove new lines, tabs, double spaces\n",
    "    s = replace_punct(s) #remove punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(s):\n",
    "    \n",
    "    \"\"\"\n",
    "    Determines sentiment scoe of a given string \n",
    "    from -1 to 1 (most negative to most positive)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s: string\n",
    "    String to be analyzed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "    Polarity score for input string\n",
    "    \"\"\"    \n",
    "    \n",
    "    score = TextBlob(s).polarity #use TextBlob package to determine tweet polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_length(tweet_df,print_count,tweet_counts):\n",
    "    \n",
    "    \"\"\"\n",
    "    Writes .csv of specified length with equal (or near equal) counts for each search term\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tweet_df: data frame\n",
    "    Data Frame of tweet data\n",
    "    \n",
    "    print_count: int\n",
    "    Total number of tweets to write\n",
    "    \n",
    "    tweet_counts: list\n",
    "    List of number of tweets of each search term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    short_df: data frame\n",
    "    Data Frame with specified amount of tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    iterations = len(tweet_counts)\n",
    "    tweets_per_search = int(np.floor(print_count/iterations))\n",
    "    extra = print_count%iterations\n",
    "    \n",
    "    short_df = pd.DataFrame()\n",
    "    short_df = tweet_df.iloc[1: (tweets_per_search + 1)]\n",
    "    counter = tweet_counts[0]\n",
    "    \n",
    "    for i in range(1,iterations):\n",
    "\n",
    "        if i == iterations - 1:\n",
    "            short_df = short_df.append(tweet_df.iloc[counter:counter + (tweets_per_search + extra)])\n",
    "        else:   \n",
    "            short_df = short_df.append(tweet_df.iloc[counter:counter + tweets_per_search])\n",
    "            counter = counter + tweet_counts[i]\n",
    "            \n",
    "    csv_name = 'tweets_' + str(print_count) + '.csv'\n",
    "    short_df.to_csv(csv_name, index=False)\n",
    "    \n",
    "    return short_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: #%23rockmusic -filter:retweets -_ArtistRack -coffeeradiogr -RockTalkFM -ZillichR\n",
      "running: #%23popmusic -filter:retweets -_ArtistRack -coffeeradiogr -YTPopMusicChan1 -Get_Heard_Today\n",
      "running: #%23countrymusic -filter:retweets -Until_Sana -videos_country -ombui -Rchemutai -ItsMwangiKelvyn\n"
     ]
    }
   ],
   "source": [
    "#search for three terms, one for each rock, pop, and country. I updated the search to remove users I deemed \"spam\"*\n",
    "search_list = ['#%23rockmusic -filter:retweets -_ArtistRack -coffeeradiogr -RockTalkFM -ZillichR',              \\\n",
    "                '#%23popmusic -filter:retweets -_ArtistRack -coffeeradiogr -YTPopMusicChan1 -Get_Heard_Today',  \\\n",
    "                '#%23countrymusic -filter:retweets -Until_Sana -videos_country -ombui -Rchemutai -ItsMwangiKelvyn']\n",
    "\n",
    "#get at least 1000 tweets for each search term\n",
    "num_tweets = 1000\n",
    "\n",
    "#get tweets\n",
    "full_tweet_list = get_tweets(search_list,num_tweets)  #fetch tweets from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['rock','pop','country']                   #define names for search terms in search_list\n",
    "tweet_dict = tweet_dict_pop(full_tweet_list,genres) #create dictionary with relevant tweet information\n",
    "tweet_df = dict_to_df(tweet_dict,genres)            #convert dictionary to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['text'] = tweet_df['text'].apply(clean_tweets)  #clean tweet text\n",
    "tweet_df['user'] = tweet_df['user'].apply(clean_tweets)  #clean tweet usernames\n",
    "tweet_df['location'] = tweet_df['location'].apply(clean_tweets)  #clean tweet locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['sentiment'] = tweet_df['text'].apply(score_sentiment)  #analyze sentiment of tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Export Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts = []\n",
    "for i in range(len(full_tweet_list)):\n",
    "    tweet_counts.append(len(full_tweet_list[i]))\n",
    "\n",
    "short_df = write_csv_length(tweet_df,100,tweet_counts)  #create .csv with 100 tweets\n",
    "short_df = write_csv_length(tweet_df,1000,tweet_counts) #create .csv with 1000 tweets\n",
    "short_df = write_csv_length(tweet_df,len(tweet_df),tweet_counts)  #create .csv with all tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To perform the searches of #rockmusic, #popmusic, and #countrymusic, I had to make several modifications instead of just searching for the base hashtags. I used #%23 for the \"hashtag\" symbol, -filter:retweets to remove retweets and obtain original tweets only, and I filtered out users I deemed to be spam. I filtered these users in several ways. First, I looked at the .csv file with 100 tweets and removed users that occured an unusal number of times, especially in a row. These users generally posted the same tweet many times in a row. I found users like this for each search. Also, using my R-code I used the location summaries to determine unusual locations with high numbers of tweets. For example, I found three users frequently using #countrymusic in the locations of Kenya, Nairobi, and Nairobi,Kenya whose tweets had no relavance to country music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
